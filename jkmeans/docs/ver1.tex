\documentclass[12pt]{article}
\pdfoutput=1 

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\openup 1em

%macro for commenting
\usepackage{color}
\newcommand{\leo}[1]{{\color{blue}{\it leo: #1}}}
\newcommand{\xbeta}{\boldsymbol X_i^T \boldsymbol\beta}


\usepackage[round]{natbib}

\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{float}

\usepackage{amsthm,amsmath} 
\usepackage{amssymb}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\thispagestyle{empty}
\baselineskip=28pt

\title
    {?? Mixture Model}


\date{}

\begin{document}
    
\maketitle

\section{Introduction}

hard clustering:
several overly strong assumptions in k-means

soft clustering:
large variance induced by way too many non-zeros in the probability simplex.

variance-bias tradeoff?
reducing variability on the weight

\section{?? Mixture Model}

\subsection{General Framework}

On a separable metrics space $\{\mathcal{X},\mathcal{F}\}$, we assume each data $y_i$ has the following probability density as the likelihood:

\begin{equation}
\begin{aligned}
& \pi(y_i) \sim G(\theta_i) \\
& \pi(\theta_i) = \sum_{k=1}^{\kappa} w_{k,i} \delta_{\theta_{k}}(\theta_i) \\
& w_{k,i} = \frac{ u_{k,i} v_{k} } { \sum_{k=1}^{\kappa}  u_{k,i} v_{k} }\\
& u_{k,i} \in \{0,1\}
\end{aligned}
\label{marginal_lik}
\end{equation}
where $\delta_{\theta_{k}}(y_i)$ is a probabilty density corresponds to a component distribution with parameter $\theta_{k}$, and $w_{k,i}$ is the component weight that varies by index $i$ and $\sum_{k=1}^{\kappa} w_{k,i} =1$. The weight varying is due to the randomness in $u_{k,i}$. Due to the $0$'s $u_{k,i}$ introduces, this induces sparsity in the membership probabilty $w_{k,i}$.

\subsection{Estimation}

We now use the latent variable $\{ z_{1,i},z_{2,i},\ldots z_{\kappa,i} \} \sim MultiNomial (\{ w_{1,i},w_{2,i},\ldots w_{\kappa,i}\} )$ that takes value of $\{0,\ldots,1,\ldots 0\}$ that assign randomly one $1$ to one of $\kappa$ vertices in the simplex. The likelihood can be rewritten as:


\begin{equation}
\begin{aligned}
& [ y_i ] =   \sum_{k=1}^{\kappa} \int z_{k,i} \delta_{\theta_{k}}(y_i) P(d z_{k,i}) = \int \sum_{k=1}^{\kappa} z_{k,i} \delta_{\theta_{k}}(y_i) P(d z_{k,i})
\end{aligned}
\end{equation}
where $P(d z_{k,i})$ is the measure of the multinomial distribution aforementioned, the last equation is due to Fubini theorem. Note due to the $z_{k,i}$ takes only one $1$ out of $K$, this allows a simple log-density augmented with $z_{k,i}$:

\begin{equation}
\begin{aligned}
& \log[ \{y_i\}_i, \{z_{k,i}\}_{k,i} ] =  \sum_{i}\sum_{k=1}^{\kappa} z_{k,i} \log \{\delta_{\theta_{k}}(y_i) v_k \}
\end{aligned}
\label{conditional_lik}
\end{equation}

 This allows us to utilize Expectation-Maximization algorithm for parameter estimation:

\begin{algorithm}[H]
\caption{EM algorithm}\label{alg:euclid}
\begin{algorithmic}[1]
% \Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
% \State $r\gets a\bmod b$
\While{$||\theta^{(s)}-\theta^{(s+1)}||>\epsilon$}%\Comment{We have the answer if r is 0}
\State Maximize over $\{ u_{i,k}\}_k$ by $ \underset{\{u_{i,k}\}_k} {\text{argmax }}\{u_{i,k}  v_{k}\delta_{\theta_{k}}(y_i)\}$ for all $i$; \Comment{\small pick top J out K based on (\ref{marginal_lik})}
\State Compute $ \mathbb{E}(z_{k,i})= \frac{w_{k,i} \delta_{\theta_{k}}(y_i)}{\sum_k w_{k,i} \delta_{\theta_{k}}(y_i)}$ with $w_{k,i} = u_{i,k} v_{k}$; \Comment{E step  based on (\ref{conditional_lik})}
\State Set $\theta^{(s+1)}_{k}= \text{argmax} \sum_{k=1}^{\kappa} \mathbb{E}z_{k,i} \log \delta_{\theta_{k}}(y_i) $ for all $k$ \Comment{M step: obtain MLE based on (\ref{conditional_lik})}
\State Set $v^{(s+1)}_{k}=  \sum_{i}\mathbb{E}z_{k,i} /  \sum_{i}\sum_{k}\mathbb{E}z_{k,i}$ \Comment{M step: obtain MLE based on (\ref{conditional_lik})}\label{flex_w_step}
\EndWhile
% \State \textbf{return} $b$\Comment{The gcd is b}
% \EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Theory}

variance reduction in the weight parameter

1. k-means is asymptotic biased
2. large k reducing variance
3. small sample theory


\section{Simulation}

large K with small J in 1d

high p setting


\section{Application}
\section{Discussion}

\bibliographystyle{plainnat}
\bibliography{reference}

\end{document}


```
###Prerequisite
###Install the package if you haven't
setwd("~/git/")
require('devtools')
build('jkmeans')
install.packages("jkmeans_1.0.tar.gz", repos = NULL, type = "source")
```

```{r include=FALSE}
require('ggplot2')
require('reshape')
require('jkmeans')

```


###1. Generate Data
Let's generate data from 2 clusters, with size n/2, n/2*3

N( c(0.7,0.3), diag(1.7661/n))
N( c(0.3,0.7), diag(1.7661/n))

```{r}
n<- 500
K<- 2
p<- 2
batchN<- 10000

yBatch<- array(0,dim = c(n*K,p,batchN))


mu0<- c(c(0.7,0.3), c(0.3,0.7))
mu<- matrix( 0,n*2,p)
mu[1:(n/2),]<- rep(c(0.7,0.3),each=(n/2))
mu[(n/2+1):(2*n),]<- rep(c(0.3,0.7),each=(n/2*3))

sigma<-  sqrt(1.7661/30)

for(b in 1:batchN){
  yBatch[,,b]<- matrix( rnorm(n*p*2,mu,sigma), n*K, p)
}


mu_ini<- matrix(mu0,K,p)


computeMError<- function(jk, n){
  error<- numeric(batchN)
  for(b in 1:batchN){
    newOrder<- order(jk$mu[,1,b],decreasing = T)
    tM0<- c(0,1)[newOrder]
    trueM<- c(rep(tM0[1],(n/2)),rep(tM0[2],(n/2*3)))
    error[b]<- sum (jk$M[,b] != trueM) /2/n
  }
  error
}

computeRMSE<- function(jk, mu0){
  rmse<- numeric(batchN)
  for(b in 1:batchN){
    newOrder<- order(jk$mu[,1,b],decreasing = T)
    
    muEst<-jk$mu[,,b][newOrder,]
    
    rmse[b]<- sqrt( mean((muEst - mu0)^2))
  }
  rmse
}

```

###2. Test jk-means with different j


Run 12-Means, get average error

```{r}


jk12 <- jkmeans::jkmeansEMBatch(yBatch, k=2, j = 1,  1000,tol = 1E-10,useKmeansIni = F, meansIni = mu_ini)


error12 <- computeMError(jk12,n)
rmse12 <- computeRMSE(jk12, mu0)

mean(error12)
mean(rmse12)
```


Run 22-Means, get average error

```{r}


jk22 <- jkmeans::jkmeansEMBatch(yBatch, k=2, j = 2,  1000,tol = 1E-10,useKmeansIni = F, meansIni = mu_ini)

error22 <- computeMError(jk22,n)
rmse22 <- computeRMSE(jk22, mu0)

mean(error22)
mean(rmse22)
```


Plot overlay error rate histograms:

```{r}

dataHist<- data.frame("J"=rep(c(1,2),each=batchN),
                  "ErrorRate"= c(error12,error22))

ggplot(dataHist, aes(ErrorRate, fill = as.factor(J))) + geom_histogram(alpha = 0.2,bins = 50,  position="identity")
```



###2. Run j-sparse-GMM with different j


Run 12-GMM, get average error

```{r}


jk12 <- jkmeans::jkmeansEMBatch(yBatch, k=2, j = 1,  1000,tol = 1E-10,useKmeansIni = F, meansIni = mu_ini,fixW = F)


error12 <- computeMError(jk12,n)
rmse12 <- computeRMSE(jk12, mu0)

mean(error12)
mean(rmse12)
```


Run 22-GMM, get average error

```{r}


jk22 <- jkmeans::jkmeansEMBatch(yBatch, k=2, j = 2,  1000,tol = 1E-10,useKmeansIni = F, meansIni = mu_ini,fixW = F)

error22 <- computeMError(jk22,n)
rmse22 <- computeRMSE(jk22, mu0)

mean(error22)
mean(rmse22)

```


Plot overlay error rate histograms:

```{r}

dataHist<- data.frame("J"=rep(c(1,2),each=batchN),
                  "ErrorRate"= c(error12,error22))

ggplot(dataHist, aes(ErrorRate, fill = as.factor(J))) + geom_histogram(alpha = 0.2,bins = 50,  position="identity")
```


#Conclusion:

The better performance of the k-means over GMM in terms of clustering, reported by Renze, is more likely due to fixing w=1/k in k-means.

On the other hand, we do see 1-sparse-GMM outperforms 2-sparse-GMM in clustering. This suggests focusing applying the sparsity idea in general mixture model would be a good direction.
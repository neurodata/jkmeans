\documentclass[12pt]{article}
\pdfoutput=1 

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\openup 1em

%macro for commenting
\usepackage{color}
\newcommand{\leo}[1]{{\color{blue}{\it leo: #1}}}
\newcommand{\xbeta}{\boldsymbol X_i^T \boldsymbol\beta}


\usepackage[round]{natbib}

\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{float}

\usepackage{amsthm,amsmath} 
\usepackage{amssymb}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}


\usepackage{mhequ}
\newcommand{\be}{\begin{equs}}
\newcommand{\ee}{\end{equs}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}

\thispagestyle{empty}
\baselineskip=28pt

\title
    {Complete Membership Model for High Dimensional Clustering}


\date{}

\begin{document}
    
\maketitle

\section{Introduction}

Literature review:

Finite mixture model clustering.

Difficulty in high dimensional clustering.

The difficulty in Determinantal process clustering, repulsive clustering.

Classification EM.

\section{Complete Membership Model}

\subsection{General Framework}

Let $y_i$ be the observed data following the distribution $F$ with parameter $\theta_i$. Consider a  membership model with $\kappa$ possible membership:

\begin{equation}
\begin{aligned}
& \pi(y_i) \stackrel{indep}{\sim} F(y_i|\theta_i) \\
& \pi(\theta_i) = \sum_{k=1}^{\kappa} z_{k,i} \delta_{\theta^*_{k}}(\theta_i) \\
\end{aligned}
\label{membership_model}
\end{equation}
for each $i$, $\{ z_{.,i}\}$ denotes the membership with only one $1$ and $(k-1)$ many $0$'s. Note $\{ z_{.,i}\}$ is not random for each $i$, although collectively $\{ \sum_i  z_{1,i}, \sum_i  z_{2,i},\ldots \sum_i  z_{\kappa,i} \}$ follows a multinomial distribution $Multinomial(n, \{ w_1, w_2, \ldots, w_\kappa \})$.
 
\subsection{Estimation}

\begin{equation}
\begin{aligned}
& \log[ \{y_i\}_i, \{z_{k,i}\}_{k,i} ] =  \sum_{i}\sum_{k=1}^{\kappa} z_{k,i} \log \{w_k F(y_i\mid \theta_k)  \}
\end{aligned}
\label{conditional_lik}
\end{equation}

 This allows us to utilize Expectation-Maximization algorithm for parameter estimation:

\begin{algorithm}[H]
\caption{Estimation algorithm}\label{alg:euclid}
\begin{algorithmic}[1]
\While{$||\theta^{(s)}-\theta^{(s+1)}||>\epsilon$}%\Comment{We have the answer if r is 0}
\State Maximize over $\{ z_{i,k}\}_k$ by $ \underset{\{z_{i,k}\}_k} {\text{argmax }}\{z_{i,k}  w_{k}F (y_i|\theta^*_k)\}$ for all $i$;
\State Update $\theta^{*}_{k}= \text{argmax} \sum_{i=1}^{n} z_{k,i} \log F(y_i|\theta^*_k) $ for all $k$ 
\State Update $w_{k}=  \sum_{i} z_{k,i} /  \sum_{i}\sum_{k}z_{k,i}$ 
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{High Dimensional Estimation}

Alternative Least Square with regularization

\section{Theory}

\subsection{Increased Separation among the Centers}

Consider the finite mixture model with the center estimates as $\mu_k = \frac{\sum_i \bb E z_{k,i} y_i}{\sum_i \bb E z_{k,i}}$, and complete membership model with the center estimates as $\mu^*_k = \frac{\sum_i  z_{k,i} y_i}{\sum_i  z_{k,i}}$. We are interested in comparing the pairwise distance among the centers from the two models.

Let the pairwise distance be $||\mu_1 - \mu_2||$ between two centers in the finite mixture. As the $||\mu_1 - \mu_2||= \sqrt{ \sum_{l=1}^{p} ||\mu_{1,l} - \mu_{2,l}||^2}$, we focus on one sub-dimension $\mu_{1,l} - \mu_{2,l}$, without loss of generality, we assume $\mu_{1,l} > \mu_{2,l}$.

For any $y_{j,l} \ge  \frac{\sum_{i\ne j} \bb E z_{1,i} y_{i,l} }{  \sum_{i\ne j} E z_{1,i} } \ge \frac{\sum_{i\ne j} \bb E z_{2,i} y_{i,l} }{  \sum_{i\ne j} E z_{2,i} } $ and $ \bb E z_{1,j} \ge \bb E z_{2,j}$,
\be
\mu_{1,l} - \mu_{2,l} = & \frac{\sum_{i\ne j} \bb E z_{1,i} y_{i,l}  + \bb E z_{1,j} y_{j,l}}{  \sum_{i\ne j} \bb E z_{1,i}  + \bb E z_{1,j} }  -  \frac{\sum_{i\ne j} \bb E z_{2,i} y_{i,l}  + \bb E z_{2,j} y_{j,l}}{  \sum_{i\ne j} \bb E z_{2,i}  + \bb E z_{2,j} } \\
 \le  & \frac{\sum_{i\ne j} \bb E z_{1,i} y_{i,l}  +  y_{j,l}}{  \sum_{i\ne j} \bb E z_{1,i}  + 1 }  -  \frac{\sum_{i\ne j} \bb E z_{2,i} y_{i,l} }{  \sum_{i\ne j} \bb E z_{2,i}  }
\ee

For any $y_{j,l} \le   \frac{\sum_{i\ne j} \bb E z_{2,i} y_{i,l} }{  \sum_{i\ne j} E z_{2,i} } \le  \frac{\sum_{i\ne j} \bb E z_{1,i} y_{i,l} }{  \sum_{i\ne j} E z_{1,i} }$ and $ \bb E z_{1,j} \le \bb E z_{2,j}$,

\be
\mu_{1,l} - \mu_{2,l}  \le  & \frac{\sum_{i\ne j} \bb E z_{1,i} y_{i,l} }{  \sum_{i\ne j} \bb E z_{1,i}  }  -  \frac{\sum_{i\ne j} \bb E z_{2,i} y_{i,l} +  y_{j,l} }{  \sum_{i\ne j} \bb E z_{2,i}+1  }
\ee

 By induction, this converts all the $E z_{k,i}$ to $z_{k,i}$ hence $\mu_{1,l} - \mu_{2,l}\le \mu^{*}_{1,l} - \mu^{*}_{2,l}$.

\subsection{Convex Relaxation}

Something similar to:

Agarwal, Alekh, Sahand Negahban, and Martin J. Wainwright. "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions." The Annals of Statistics (2012): 1171-1197.

\section{Simulation}

Note: preliminary

RMSE:

\begin{table}[H]
\centering
	\begin{tabular}{| l | r | r |}
	\hline
			Model & $n=100, p=100, p^*=5$ &  $n=100, p=100, p^*=100$\\
	\hline
		k-means &  0.40 & 0.069\\
		CM		& 0.43 & 0.069\\
		GMM &  0.46 & 0.069 \\
		PCA+ k-means & 0.17 & 0.030\\
		PCA + CM		& 0.07 & 0.030\\
		PCA + GMM & 0.10 & 0.030\\
		new model  & \\
		new model &\\
		new model &\\
			\hline
	\end{tabular}
\end{table}


\section{Application}
\section{Discussion}

\bibliographystyle{plainnat}
\bibliography{reference}

\end{document}
